{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMhP1lkVEBTR"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timezone\n",
        "import os\n",
        "import dotenv\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load environment variables\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "# Constants\n",
        "MAX_RETRIES = int(os.environ.get(\"MAX_RETRIES\", \"3\"))\n",
        "RETRY_DELAY = int(os.environ.get(\"RETRY_DELAY\", \"5\"))\n",
        "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
        "AZURE_OPENAI_KEY = os.getenv(\"AZURE_OPENAI_KEY\")\n",
        "DEPLOYMENT_NAME = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "\n",
        "# Model settings\n",
        "TEMPERATURE = float(os.getenv(\"TEMPERATURE\", 0.2))\n",
        "TOP_P = float(os.getenv(\"TOP_P\", 0.2))\n",
        "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"gpt-4o\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "def call_llm_service(prompt, response_format=\"text\"):\n",
        "    client = openai.AzureOpenAI(\n",
        "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
        "        api_key=AZURE_OPENAI_KEY,\n",
        "        api_version=AZURE_OPENAI_API_VERSION,\n",
        "    )\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": prompt}]\n",
        "    if response_format == \"json\":\n",
        "        messages[0][\"content\"] += \"\\nPlease provide the response strictly in JSON format.\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=messages,\n",
        "        max_tokens=4096,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n"
      ],
      "metadata": {
        "id": "Sd1EJ59DEI5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def load_text_file(filepath, fallback=\"\"):\n",
        "    try:\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "            return file.read()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading {filepath}: {e}\")\n",
        "        return fallback\n",
        "\n",
        "def extract_pdf_file(filepath):\n",
        "    try:\n",
        "        with fitz.open(filepath) as pdf:\n",
        "            return \"\\n\".join(page.get_text() for page in pdf)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error reading PDF {filepath}: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "UtststIUEI8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load base context and generator prompt templates\n",
        "context_prompt = load_text_file(\"path/to/context_prompt.txt\")\n",
        "generator_prompt_template = load_text_file(\"path/to/generator_prompt.txt\")\n",
        "\n",
        "# Load business or solution documents (PDFs)\n",
        "business_doc = extract_pdf_file(\"path/to/business_doc.pdf\")\n",
        "\n",
        "# Load user story\n",
        "user_story = load_text_file(\"path/to/user_story.txt\")"
      ],
      "metadata": {
        "id": "EKiC0ksWEI_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_prompt_filled = generator_prompt_template.format(\n",
        "    context=context_prompt,\n",
        "    All_source_documents_with_titles=formatted_docs\n",
        ")\n",
        "\n",
        "generator_output = call_llm_service(generator_prompt_filled)\n",
        "print(\" Generator Output:\\n\", generator_output)"
      ],
      "metadata": {
        "id": "aRjON4qyEJBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4WEi3dVjEJEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZLhXycQ8EJH6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}