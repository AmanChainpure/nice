{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMhP1lkVEBTR"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timezone\n",
        "import os\n",
        "import dotenv\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Load environment variables\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "# Constants\n",
        "MAX_RETRIES = int(os.environ.get(\"MAX_RETRIES\", \"3\"))\n",
        "RETRY_DELAY = int(os.environ.get(\"RETRY_DELAY\", \"5\"))\n",
        "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
        "AZURE_OPENAI_KEY = os.getenv(\"AZURE_OPENAI_KEY\")\n",
        "DEPLOYMENT_NAME = os.getenv(\"DEPLOYMENT_NAME\")\n",
        "\n",
        "# Model settings\n",
        "TEMPERATURE = float(os.getenv(\"TEMPERATURE\", 0.2))\n",
        "TOP_P = float(os.getenv(\"TOP_P\", 0.2))\n",
        "MODEL_NAME = os.getenv(\"MODEL_NAME\", \"gpt-4o\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "def call_llm_service(prompt, response_format=\"text\"):\n",
        "    client = openai.AzureOpenAI(\n",
        "        azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
        "        api_key=AZURE_OPENAI_KEY,\n",
        "        api_version=AZURE_OPENAI_API_VERSION,\n",
        "    )\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": prompt}]\n",
        "    if response_format == \"json\":\n",
        "        messages[0][\"content\"] += \"\\nPlease provide the response strictly in JSON format.\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=messages,\n",
        "        max_tokens=4096,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n"
      ],
      "metadata": {
        "id": "Sd1EJ59DEI5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_pdf_file(file_path):\n",
        "    content = \"\"\n",
        "    try:\n",
        "        with fitz.open(file_path) as pdf_doc:\n",
        "            for page_num in range(len(pdf_doc)):\n",
        "                page = pdf_doc.load_page(page_num)\n",
        "                content += page.get_text()\n",
        "        logging.info(f\"Extracted PDF: {file_path}\")\n",
        "    except Exception as pdf_err:\n",
        "        logging.error(f\"Error extracting text from PDF: {pdf_err}\")\n",
        "    return content\n",
        "\n",
        "def load_text_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            return file.read()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to read file {file_path}: {e}\")\n",
        "        return \"\"\n"
      ],
      "metadata": {
        "id": "UtststIUEI8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_test_cases(context_prompt, user_story, reference_docs):\n",
        "    combined_prompt = f\"\"\"\n",
        "You are an expert QA engineer.\n",
        "\n",
        "Context:\n",
        "{context_prompt}\n",
        "\n",
        "User Story:\n",
        "{user_story}\n",
        "\n",
        "Reference Documents:\n",
        "{reference_docs}\n",
        "\n",
        "Task:\n",
        "Based on the above context, user story, and supporting documents, generate a comprehensive list of automated test cases.\n",
        "Include edge cases, input validation, negative and positive paths.\n",
        "Provide output in clear bullet points with expected results.\n",
        "\"\"\"\n",
        "    return call_llm_service(combined_prompt)\n"
      ],
      "metadata": {
        "id": "EKiC0ksWEI_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths - update as needed\n",
        "context_path = \"\"\n",
        "user_story_path = \"\"\n",
        "reference_doc_path = \"\"\n",
        "\n",
        "# Load content\n",
        "context_prompt = load_text_file(context_path)\n",
        "user_story = load_text_file(user_story_path)\n",
        "reference_docs = extract_pdf_file(reference_doc_path)\n",
        "\n",
        "# Generate test cases\n",
        "test_case_output = generate_test_cases(context_prompt, user_story, reference_docs)\n",
        "\n",
        "# Show result\n",
        "print(\"Generated Test Cases:\\n\")\n",
        "print(test_case_output)\n"
      ],
      "metadata": {
        "id": "aRjON4qyEJBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4WEi3dVjEJEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZLhXycQ8EJH6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}