def test_llm_connection():
    from openai import AzureOpenAI

    client = AzureOpenAI(
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
        api_key=os.getenv("AZURE_OPENAI_KEY"),
        api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
    )

    try:
        response = client.chat.completions.create(
            deployment_id=os.getenv("DEPLOYMENT_NAME"),
            messages=[{"role": "user", "content": "Hi"}],
            max_tokens=100,
            temperature=0,
        )
        print(" LLM Response:\n", response.choices[0].message.content.strip())
    except Exception as e:
        print(" Error during LLM call:", e)

# Run the test
test_llm_connection()
